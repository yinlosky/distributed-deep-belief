The process of running machine learning algorithms against very large datasets is attractive and increasingly necessary in furthering the progress of the field and producing actual solutions to large problems. For these applications, nonparametric models – models which do not grow with the size of the data – seem to be the only feasible solutions to distributed machine learning applications. However,  there is another set of models called generative models which can reconstruct the input data from the model by sampling. In a sense, one can consider generative models to be lossy compression/decompression models of the entire training set.

Neural networks with latent variables are an incredibly interesting set of generative models in this regard, because they store all observed training in a matrix of interactions between the data vector and the interaction terms. This makes them very fast to operate at test time on a query: returning results at a rate lower than network latency could distribute them. So called product-of-expert models also have potential to be more accurate than other ensemble classification methods utilizing mixture models, in simple terms because sums dilute sharp peaks while products preserve them. In addition to all of these benefits, neural networks are entirely data agnostic in that no assumptions about the input data are built into the model. Any scaled vector can be used as an input to the model, making them applicable to most numeric classification tasks.

The problem remains of how to train these systems on big data, because due to the extremely iterative nature of gradient descent, the training time scales linearly (maybe superlinearly due to the amortized cost of handling the data). In order to train these systems in a reasonable time frame, parallel implementations are desperately needed, especially since the rate of advancement of single hardware speeds has slowed and been focused towards more concurrency along multiple processors.

As a distributed framework for doing arbitrary handling of data on a distributed file system, Hadoop seems like a step in the right direction toward achieving distributed training. Our goal is to produce a trainable implementation of a deep belief network (DBN) – a stacked series of latent neural networks – on Hadoop to begin to advance this area.